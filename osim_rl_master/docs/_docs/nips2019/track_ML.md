---
title: ML Track
---

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

The Machine Learning Track is to award the team that conducts and reports the most novel machine learning study using our simulation environment.
To win this track, one should:
* rank in the top 50 of Round 1
* submit a paper describing your machine learning approach in [NeurIPS format](https://media.neurips.cc/Conferences/NeurIPS2019/Styles/neurips_2019.pdf)


Our review board led by Xue Bin (Jason) Peng, [xbpeng@berkeley.edu](mailto: xbpeng@berkeley.edu){:target='_blank'}) will select the paper that presents the most novel machine learning study using our simulation environment. The topics of interest include but are not limited to:
* **reinforcement learning** (e.g. novel RL algorithms or improvements to existing techniques [Kakade, 2001; Silver et al., 2014; Haarnoja et al., 2018])
* **imitation learning** (e.g. methods for learning from demonstrations [Peng et al., 2018; Ho and Ermon, 2016])
* **reward functions** (e.g. designing reward function that encourage natural behaviours [Wang et al., 2009; Yu et al., 2018])
* **control structures** (e.g. task-specific control structures for motion control [Wang et al., 2018; Lee et al., 2019])


The prizes for the winner are:
* 2x NVIDIA GPU
* Travel grant (up to $1,000 to NeurIPS 2019 or Stanford)
* Paper acceptance into the NeurIPS Deep RL workshop


### References
* Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Proceedings of the 35th International Conference on Machine Learning (ICML 2018).
* Ho, J., & Ermon, S. (2016). Generative Adversarial Imitation Learning. Advances in Neural Information Processing Systems (NIPS 2016).
* Kakade, S. (2001). A Natural Policy Gradient. Proceedings of the 14th International Conference on Neural Information Processing Systems (NIPS 2001).
* Lee, S., Lee, K., Park, M., and Lee, J. (2019). Scalable Muscle-actuated Human Simulation and Control. ACM Transactions on Graphics (Proc. SIGGRAPH 2019).
* Peng, X. B., Abbeel, P., Levine, S., & van de Panne, M. (2018). Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions on Graphics (Proc. SIGGRAPH 2018).
* Silver, D., Lever, G., Heess, N., Degris, T., Weirstra, D., & Riedmiller, M. (2014). Deterministic Policy Gradient Algorithms. Proceedings of the 31st International Conference on Machine Learning (ICML 2014).
* Yu, W., Turk, G., & Liu, C.K. (2018). Learning Symmetric and Low-Energy Locomotion. ACM Transactions on Graphics (Proc. SIGGRAPH 2018).
* Wang, J., Fleet, D., & Hertzmann, A. (2009). Optimizing Walking Controllers. ACM Transactions on Graphics (Proc. SIGGRAPH Asia 2009).
* Wang, T., Liao, R., Ba, J., & Fidler, S. (2018). NerveNet: Learning Structured Policy with Graph Neural Networks. International Conference on Learning Representations (ICLR 2019).
